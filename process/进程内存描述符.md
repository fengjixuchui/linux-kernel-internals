### 前言
linux进程结构体`task_struct`里有两个字段，关于进程内存空间描述的，之前没有仔细分析过，最近在分析`prctl`的时候，`PR_SET_MM`选项就是针对进程内存空间的修改，顺道研究了。
```c
	struct mm_struct		*mm;
	struct mm_struct		*active_mm;
```

### task_stuct->mm_struct
mm_struct结构在定义在`mm_types.h`
```c
struct mm_struct {
	struct {
		struct vm_area_struct *mmap;		/* list of VMAs */


		unsigned long mmap_base;	/* base of mmap area */
		unsigned long mmap_legacy_base;	/* base of mmap area in bottom-up allocations */

		unsigned long task_size;	/* size of task vm space */
		unsigned long highest_vm_end;	/* highest vma end address */
		pgd_t * pgd;



		/**
		 * @mm_users: The number of users including userspace.
		 *
		 * Use mmget()/mmget_not_zero()/mmput() to modify. When this
		 * drops to 0 (i.e. when the task exits and there are no other
		 * temporary reference holders), we also release a reference on
		 * @mm_count (which may then free the &struct mm_struct if
		 * @mm_count also drops to 0).
		 */
		atomic_t mm_users;

		/**
		 * @mm_count: The number of references to &struct mm_struct
		 * (@mm_users count as 1).
		 *
		 * Use mmgrab()/mmdrop() to modify. When this drops to 0, the
		 * &struct mm_struct is freed.
		 */
		atomic_t mm_count;

		int map_count;			/* number of VMAs */

		spinlock_t page_table_lock; /* Protects page tables and some
					     * counters
					     */
		/*
		 * With some kernel config, the current mmap_lock's offset
		 * inside 'mm_struct' is at 0x120, which is very optimal, as
		 * its two hot fields 'count' and 'owner' sit in 2 different
		 * cachelines,  and when mmap_lock is highly contended, both
		 * of the 2 fields will be accessed frequently, current layout
		 * will help to reduce cache bouncing.
		 *
		 * So please be careful with adding new fields before
		 * mmap_lock, which can easily push the 2 fields into one
		 * cacheline.
		 */
		struct rw_semaphore mmap_lock;

		struct list_head mmlist; /* List of maybe swapped mm's.	These
					  * are globally strung together off
					  * init_mm.mmlist, and are protected
					  * by mmlist_lock
					  */


		unsigned long hiwater_rss; /* High-watermark of RSS usage */
		unsigned long hiwater_vm;  /* High-water virtual memory usage */

		unsigned long total_vm;	   /* Total pages mapped */
		unsigned long locked_vm;   /* Pages that have PG_mlocked set */
		atomic64_t    pinned_vm;   /* Refcount permanently increased */
		unsigned long data_vm;	   /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
		unsigned long exec_vm;	   /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
		unsigned long stack_vm;	   /* VM_STACK */
		unsigned long def_flags;

		/**
		 * @write_protect_seq: Locked when any thread is write
		 * protecting pages mapped by this mm to enforce a later COW,
		 * for instance during page table copying for fork().
		 */
		seqcount_t write_protect_seq;

		spinlock_t arg_lock; /* protect the below fields */

		unsigned long start_code, end_code, start_data, end_data;
		unsigned long start_brk, brk, start_stack;
		unsigned long arg_start, arg_end, env_start, env_end;

		unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */

		/*
		 * Special counters, in some configurations protected by the
		 * page_table_lock, in other configurations by being atomic.
		 */
		struct mm_rss_stat rss_stat;

		struct linux_binfmt *binfmt;

		/* Architecture-specific MM context */
		mm_context_t context;

		unsigned long flags; /* Must use atomic bitops to access */

		struct user_namespace *user_ns;

		/* store ref to file /proc/<pid>/exe symlink points to */
		struct file __rcu *exe_file;

		/*
		 * An operation with batched TLB flushing is going on. Anything
		 * that can move process memory needs to flush the TLB when
		 * moving a PROT_NONE or PROT_NUMA mapped page.
		 */
		atomic_t tlb_flush_pending;

		struct uprobes_state uprobes_state;

		struct work_struct async_put_work;


	} __randomize_layout;

	/*
	 * The mm_cpumask needs to be at the end of mm_struct, because it
	 * is dynamically sized based on nr_cpu_ids.
	 */
	unsigned long cpu_bitmap[];
};
```
成员主要可以分为几类：
* 进程空间的不同内存段地址范围：
  * mmap：进程维护的vma列表
  * mmap_base: mmap Segment
  * start_brk/brk： Heap Segment的起始和结束地址
  * start_code/end_code: .text Segment的起始和结束地址
  * start_stack/task_size: stack Segment起始地址和stack大小
  * start_data/end_data: Data Segment起始地址和结束地址

这一部分抽象了进程的地址空间
![](images/mm_p.png)

* 进程/proc/pid下属性：
  * arg_start/arg_end：进程的参数起始和结束地址
  * env_start/env_end：进程的环境变量起始和结束地址
  * saved_auxv：指向/proc/pid/auxv
  * exe_file：指向/proc/pid/exe符号链接所指向的文件路径（进程文件路径）


* 内存管理
  * mm_users: 进程数量（在多线程情况下适用）
  * mm_count：mm_struct的引用计数（为0时表示可以释放）
这两个计数器的作用看起来差不多，但是具体有什么区别，可以从代码中一窥究竟
```c
static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
{
	struct mm_struct *mm, *oldmm;

	tsk->min_flt = tsk->maj_flt = 0;
	tsk->nvcsw = tsk->nivcsw = 0;
#ifdef CONFIG_DETECT_HUNG_TASK
	tsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;
	tsk->last_switch_time = 0;
#endif

	tsk->mm = NULL;
	tsk->active_mm = NULL;

	/*
	 * Are we cloning a kernel thread?
	 *
	 * We need to steal a active VM for that..
	 */
	oldmm = current->mm;
	if (!oldmm)
		return 0;

	/* initialize the new vmacache entries */
	vmacache_flush(tsk);

	if (clone_flags & CLONE_VM) {
		mmget(oldmm);
		mm = oldmm;
	} else {
		mm = dup_mm(tsk, current->mm);
		if (!mm)
			return -ENOMEM;
	}

	tsk->mm = mm;
	tsk->active_mm = mm;
	return 0;

static inline void mmget(struct mm_struct *mm)
{
	atomic_inc(&mm->mm_users);
}
}
```

当我们调用`vfork/clone`时，如果传入了`CLONE_VM`flag，标识父子进程共享同一个进程地址空间，也就是新建的进程其实是线程，此时只需要将`oldmm->mm_users`+1，换言之，`mm_users`其实标识了当前进程的`thread`数量。

这里面有一处`oldmm`的检查是由于在Linux中，用户进程和内核线程都是`task_struct`实例，区别在于内核线程没有进程地址空间，也就没有`mm`描述符（为NULL）。但是问题来了，内核线程`task->mm`为NULL，不需要访问用户进程地址空间，但是他需要访问内核地址空间，需要用到page table，
内核根据`task->mm`是否为NULL判断是否是内核线程。好在对用户进程来说，它们的内核空间是相同的，因此内核可以用上一个被调度的用户进程的`mm`页表来访问内核地址，这个`mm`就记录在`active_mm`。
简言之，`task->mm==NULL`表内核线程身份，`task->active_mm`是借用的上一个用户进程的`mm`;对于用户进程`task->mm == task->active_mm`
而`mm_count`计数就表示`mm_struct`被内核线程引用的次数，只有当`mm_count`为0时，`mm_struct`才会被销毁。（并不是进程退出后，`mm_struct`会立即销毁）


```c
/*
 * context_switch - switch to the new MM and the new thread's register state.
 */
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
	       struct task_struct *next, struct rq_flags *rf)
{
	prepare_task_switch(rq, prev, next);

	/*
	 * For paravirt, this is coupled with an exit in switch_to to
	 * combine the page table reload and the switch backend into
	 * one hypercall.
	 */
	arch_start_context_switch(prev);

	/*
	 * kernel -> kernel   lazy + transfer active
	 *   user -> kernel   lazy + mmgrab() active
	 *
	 * kernel ->   user   switch + mmdrop() active
	 *   user ->   user   switch
	 */
	if (!next->mm) {                                // to kernel
		enter_lazy_tlb(prev->active_mm, next);

		next->active_mm = prev->active_mm;
		if (prev->mm)                           // from user
			mmgrab(prev->active_mm);
		else
			prev->active_mm = NULL;
	} else {                                        // to user
		membarrier_switch_mm(rq, prev->active_mm, next->mm);
		/*
		 * sys_membarrier() requires an smp_mb() between setting
		 * rq->curr / membarrier_switch_mm() and returning to userspace.
		 *
		 * The below provides this either through switch_mm(), or in
		 * case 'prev->active_mm == next->mm' through
		 * finish_task_switch()'s mmdrop().
		 */
		switch_mm_irqs_off(prev->active_mm, next->mm, next);

		if (!prev->mm) {                        // from kernel
			/* will mmdrop() in finish_task_switch(). */
			rq->prev_mm = prev->active_mm;
			prev->active_mm = NULL;
		}
	}
    ...
}

```
内核进程调度时，如果`next->mm`为NULL（表示切换到内核线程），则`next->active_mm = prev->active_mm`借用上一个线程的`mm`，
如果上一个线程是用户线程，则`mm->mm_count`加1.


### 
* https://www.cnblogs.com/MrLiuZF/p/15150009.html
